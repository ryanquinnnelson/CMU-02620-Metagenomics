{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "academic-publisher",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "front-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.metagenomics import sampling2, encoding2\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import csv\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-stick",
   "metadata": {},
   "source": [
    "### Ideas for model improvement\n",
    "- consider setting class weights to reflect unequal distribution of data\n",
    "- try one-vs-all approach\n",
    "- encode sequences differently depending on whether sequence is a promoter or gene vs CpG islands, etc. Search for known motifs rather than blind k-mer groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-worst",
   "metadata": {},
   "source": [
    "# Recall score\n",
    "From the paper: \"Performance is measured in terms of species-level recall. We first compute the prediction recall within each species, i.e. the proportion of fragments originating from this species that are correctly classified and consider the average recall observed across species.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unauthorized-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 1, 1]\n",
    "\n",
    "recall_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-respondent",
   "metadata": {},
   "source": [
    "# Grid Search 1 - 2000 size dataset (4 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-liabilities",
   "metadata": {},
   "source": [
    "### Run Grid Search 1 - Multiclass Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "consecutive-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results_to_file(filename, fields=None, rows=None):\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        if fields:\n",
    "            write.writerow(fields)\n",
    "\n",
    "        if rows:\n",
    "            write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "infectious-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is subset accuracy.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    score = rf.score(X_test, y_test)\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "driving-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is species level recall.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "preceding-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed):\n",
    "    # delete output directory if it previously exists\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except FileNotFoundError:\n",
    "        print('Existing directory was not found. Process will generate a directory.')\n",
    "\n",
    "    # build fragments\n",
    "    sampling2.generate_fragment_data(seq_file, taxid_file, output_dir, sample_length, coverage, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lesser-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fragments(output_dir, pattern, k, seed):\n",
    "    \n",
    "    # encode data\n",
    "    fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "    X_enc, y = encoding2.encode_fragment_dataset(fragments,k)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    # split data into test and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.33, random_state=seed)\n",
    "    \n",
    "    print(X_enc.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "touched-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators):\n",
    "    n = len(list_sample_length) * len(list_coverage) * len(list_k) *len(list_max_depth) * len(list_n_estimators)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "endangered-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_rf(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_max_depth, \n",
    "                              list_n_estimators, \n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                             score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    for sample_length in list_sample_length:\n",
    "        for coverage in list_coverage:\n",
    "            \n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            for k in list_k:\n",
    "                \n",
    "                # kmer combination\n",
    "                X_train, X_test, y_train, y_test = encode_fragments(output_dir, pattern,k,seed)\n",
    "                for max_depth in list_max_depth:\n",
    "                    for n_estimators in list_n_estimators:\n",
    "                        \n",
    "                        # random forest combination\n",
    "                        score = run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # output results to file\n",
    "                        row = [experiment, 'multiclass', 'Random Forest', X_train.shape, sample_length, coverage, k, max_depth, n_estimators, score, score_type]\n",
    "                        append_results_to_file(grid_search_file, row)\n",
    "                        \n",
    "                print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fewer-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hyperparameter_relationship(filename):\n",
    "    \"\"\"\n",
    "    Runs logistic regression over hyperparameters to find the regression coefficients.\n",
    "    This should give some indicator of how hyperparameters are affecting the score.\n",
    "    \"\"\"\n",
    "    # read in grid search results\n",
    "    df = pd.read_csv(filename)\n",
    "    X = df.drop(['experiment','score', 'category','classifier'],axis=1)\n",
    "    y = df['score']\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "    return lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-partner",
   "metadata": {},
   "source": [
    "### Search 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "affected-logan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1, 1, 2, 10]\n",
    "# list_k = [i for i in range(1,10,2)]\n",
    "# list_max_depth = [i for i in range(2,20,4)]\n",
    "# list_n_estimators = [i for i in range(20,200,40)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "common-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.07.15.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.44909382e-04,  1.67645287e-02, -9.17412031e-03,  1.18482770e-03, -1.44167915e-05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-office",
   "metadata": {},
   "source": [
    "### Search 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "loving-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# 3h 37min 3s\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200, 400]\n",
    "# list_coverage = [1,10,100,200]\n",
    "# list_k = [i for i in range(1,20,2)]\n",
    "# list_max_depth = [i for i in range(6,20,2)]\n",
    "# list_n_estimators = [i for i in range(50,501,50)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "closing-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.31.24.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.79147764e-04,  8.02651943e-04, -1.15739138e-02,  2.46285125e-03, 4.98644276e-06])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-heritage",
   "metadata": {},
   "source": [
    "### Search 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dental-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 2h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.3'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1,1,10,100,200]\n",
    "# list_k = [1,2,4,8]\n",
    "# list_max_depth = [i for i in range(20,51,10)]\n",
    "# list_n_estimators = [50,100,200,400,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-professor",
   "metadata": {},
   "source": [
    "### Search 1.4\n",
    "Stopped short after realizing I didn't need to try as small intervals of max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "distributed-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.4'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [50]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-newsletter",
   "metadata": {},
   "source": [
    "### Search 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tutorial-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 5.5 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.5'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6]\n",
    "# list_max_depth = [50, 100, 200]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-seeking",
   "metadata": {},
   "source": [
    "### Search 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fixed-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #2.25 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.6'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,3]\n",
    "# list_max_depth = [55, 65, 75, 85, 95]\n",
    "# list_n_estimators = [400,500,600,700,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "known-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi-all.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.95530967e-04,  1.27461350e-03, -1.10693995e-02,  1.45827089e-03, 4.77729253e-05])\n",
    "# # x1000 =\n",
    "# # -0.19  sample length\n",
    "# #  1.27  coverage\n",
    "# # -10    k\n",
    "# #  1.45  max_depth\n",
    "# #  0.047 n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-carbon",
   "metadata": {},
   "source": [
    "### Search 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alike-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.7'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,501, 50)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-frequency",
   "metadata": {},
   "source": [
    "### Search 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "covered-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 9 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.8'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,101, 10)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-indication",
   "metadata": {},
   "source": [
    "### Search 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "proud-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 51 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.9'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(10,101, 2)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-shore",
   "metadata": {},
   "source": [
    "### Search 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "proof-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 16 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.10'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(2,103,10)] + [i for i in range(100,1001,100)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-bouquet",
   "metadata": {},
   "source": [
    "### Search 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "possible-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.11'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(200,501,25)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-sender",
   "metadata": {},
   "source": [
    "### Search 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "molecular-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1h\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.12'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12,14,16,18,20]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-interstate",
   "metadata": {},
   "source": [
    "### Search 1.13\n",
    "Searching over smaller max_depth and n_estimator space due to limitations as we scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "blind-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 22 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.13'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [i for i in range(5,30,5)]\n",
    "# list_n_estimators = [i for i in range(20,50,5)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-latest",
   "metadata": {},
   "source": [
    "### Search 1.14\n",
    "Switching to species-level recall and rechecking highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "virtual-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32564, 400)\n",
      "Percent complete: 100.0\n",
      "CPU times: user 23.3 s, sys: 210 ms, total: 23.6 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '1.14'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [400]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-prairie",
   "metadata": {},
   "source": [
    "# Grid Search 2 - 10000 size dataset (198 examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-detection",
   "metadata": {},
   "source": [
    "### Search 2.01\n",
    "Determine if method even works with new data.\n",
    "Result: takes a long time to complete even a single run with 500 estimators and 72 max depth. May need to reduce one or both - reconsider how grid search 1 performs with higher coverage but lower estimators / depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "administrative-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 47 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.01'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-contract",
   "metadata": {},
   "source": [
    "### Search 2.02\n",
    "10x less estimators. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedicated-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 5 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.02'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-bridge",
   "metadata": {},
   "source": [
    "### Search 2.03\n",
    "10x less depth. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fancy-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.03'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [7]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-pathology",
   "metadata": {},
   "source": [
    "### Search 2.04\n",
    "Stopped early because we don't need to try k values higher than 4 because the accuracy is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accomplished-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.04'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-thunder",
   "metadata": {},
   "source": [
    "### Search 2.05\n",
    "Stopped after first run at 400x sample coverage. Took at least 10 hours for that run alone, maybe longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "signed-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.05'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200,400]\n",
    "# list_coverage = [10,50,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-opera",
   "metadata": {},
   "source": [
    "### Search 2.06\n",
    "Testing with small max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thirty-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 10 h 15 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.06'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-bridges",
   "metadata": {},
   "source": [
    "# Real Runs - 10000 size dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-walker",
   "metadata": {},
   "source": [
    "### Run 3.01\n",
    "Testing with small max_depth and n_estimators to get initial round of data for diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 400)\n",
      "Percent complete: 0.9523809523809524\n",
      "(1339, 800)\n",
      "Percent complete: 1.9047619047619049\n",
      "(1339, 6248)\n",
      "Percent complete: 2.857142857142857\n",
      "(1339, 16953)\n",
      "Percent complete: 3.8095238095238098\n",
      "(1339, 15609)\n",
      "Percent complete: 4.761904761904762\n",
      "(1339, 13287)\n",
      "Percent complete: 5.714285714285714\n",
      "(1339, 10647)\n",
      "Percent complete: 6.666666666666667\n",
      "(12432, 400)\n",
      "Percent complete: 7.6190476190476195\n",
      "(12432, 800)\n",
      "Percent complete: 8.571428571428571\n",
      "(12432, 6400)\n",
      "Percent complete: 9.523809523809524\n",
      "(12432, 56642)\n",
      "Percent complete: 10.476190476190476\n",
      "(12432, 121486)\n",
      "Percent complete: 11.428571428571429\n",
      "(12432, 117394)\n",
      "Percent complete: 12.380952380952381\n",
      "(12432, 95302)\n",
      "Percent complete: 13.333333333333334\n",
      "(123485, 400)\n",
      "Percent complete: 14.285714285714285\n",
      "(123485, 800)\n",
      "Percent complete: 15.238095238095239\n",
      "(123485, 6400)\n",
      "Percent complete: 16.19047619047619\n",
      "(123485, 65530)\n",
      "Percent complete: 17.142857142857142\n",
      "(123485, 509744)\n",
      "Percent complete: 18.095238095238095\n",
      "(123485, 853683)\n",
      "Percent complete: 19.047619047619047\n",
      "(123485, 754554)\n",
      "Percent complete: 20.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parameters\n",
    "seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "output_dir = 'data/sampling/sampling-toy-10000'\n",
    "pattern = 'fragments*.npy'\n",
    "seed = None\n",
    "date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "experiment = '3.01'\n",
    "score_type = 'species_recall'\n",
    "\n",
    "# combinations to try\n",
    "list_sample_length = [100,200,400]\n",
    "list_coverage = [0.1,1,10,100,400]\n",
    "list_k = [1,2,4,6,8,10,12]\n",
    "list_max_depth = [15]\n",
    "list_n_estimators = [50]\n",
    "\n",
    "\n",
    "grid_search_multiclass_rf(seq_file, \n",
    "                          taxid_file, \n",
    "                          output_dir, \n",
    "                          pattern, \n",
    "                          list_sample_length, \n",
    "                          list_coverage, \n",
    "                          list_k, \n",
    "                          list_max_depth,\n",
    "                          list_n_estimators, \n",
    "                          seed,\n",
    "                          grid_search_file,\n",
    "                          fields,\n",
    "                          experiment,\n",
    "                          score_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-playlist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
