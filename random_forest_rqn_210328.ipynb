{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-lender",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.metagenomics import sampling2, encoding2\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import csv\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-magic",
   "metadata": {},
   "source": [
    "### Ideas for model improvement\n",
    "- consider setting class weights to reflect unequal distribution of data\n",
    "- try one-vs-all approach\n",
    "- encode sequences differently depending on whether sequence is a promoter or gene vs CpG islands, etc. Search for known motifs rather than blind k-mer groups.\n",
    "- use PCA to reduce dimensions first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-challenge",
   "metadata": {},
   "source": [
    "# Recall score\n",
    "From the paper: \"Performance is measured in terms of species-level recall. We first compute the prediction recall within each species, i.e. the proportion of fragments originating from this species that are correctly classified and consider the average recall observed across species.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accurate-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 1, 1]\n",
    "\n",
    "recall_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-google",
   "metadata": {},
   "source": [
    "# Grid Search 1 - 2000 size dataset (4 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-symphony",
   "metadata": {},
   "source": [
    "### Run Grid Search 1 - Multiclass Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cloudy-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results_to_file(filename, fields=None, rows=None):\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        if fields:\n",
    "            write.writerow(fields)\n",
    "\n",
    "        if rows:\n",
    "            write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intensive-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is subset accuracy.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    score = rf.score(X_test, y_test)\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sporting-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is species level recall.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "front-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed):\n",
    "    # delete output directory if it previously exists\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except FileNotFoundError:\n",
    "        print('Existing directory was not found. Process will generate a directory.')\n",
    "\n",
    "    # build fragments\n",
    "    sampling2.generate_fragment_data(seq_file, taxid_file, output_dir, sample_length, coverage, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fourth-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fragments(output_dir, pattern, k, seed):\n",
    "    \n",
    "    # encode data\n",
    "    fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "    X_enc, y = encoding2.encode_fragment_dataset(fragments,k)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    # split data into test and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.33, random_state=seed)\n",
    "    \n",
    "    print(X_enc.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "martial-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators):\n",
    "    n = len(list_sample_length) * len(list_coverage) * len(list_k) *len(list_max_depth) * len(list_n_estimators)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respective-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_rf(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_max_depth, \n",
    "                              list_n_estimators, \n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                             score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    for sample_length in list_sample_length:\n",
    "        for coverage in list_coverage:\n",
    "            \n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            for k in list_k:\n",
    "                \n",
    "                # kmer combination\n",
    "                X_train, X_test, y_train, y_test = encode_fragments(output_dir, pattern,k,seed)\n",
    "                for max_depth in list_max_depth:\n",
    "                    for n_estimators in list_n_estimators:\n",
    "                        \n",
    "                        # random forest combination\n",
    "                        score = run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # output results to file\n",
    "                        row = [experiment, 'multiclass', 'Random Forest', X_train.shape, sample_length, coverage, k, max_depth, n_estimators, score, score_type]\n",
    "                        append_results_to_file(grid_search_file, row)\n",
    "                        \n",
    "                print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "banner-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hyperparameter_relationship(filename):\n",
    "    \"\"\"\n",
    "    Runs logistic regression over hyperparameters to find the regression coefficients.\n",
    "    This should give some indicator of how hyperparameters are affecting the score.\n",
    "    \"\"\"\n",
    "    # read in grid search results\n",
    "    df = pd.read_csv(filename)\n",
    "    X = df.drop(['experiment','score', 'category','classifier'],axis=1)\n",
    "    y = df['score']\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "    return lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rural-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(sample_length, coverage, df):\n",
    "    data = df[(df['sample_length'] == sample_length) & (df['coverage']==coverage)].drop(['sample_length', 'coverage'],axis=1)\n",
    "    return data['k'].tolist(), data['score'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordered-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.drop(['experiment','category','classifier','training shape', 'score type', 'max_depth','n_estimators'],axis=1)\n",
    "   \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "#     fig.tight_layout()\n",
    "\n",
    "    # graph 1\n",
    "    L=100\n",
    "    line_L100_C0_1_k, line_L100_C0_1_score = get_line(L, 0.1, df)\n",
    "    line_L100_C1_k, line_L100_C1_score = get_line(L, 1, df)\n",
    "    line_L100_C10_k, line_L100_C10_score = get_line(L, 10, df)\n",
    "    line_L100_C100_k, line_L100_C100_score = get_line(L, 100, df)\n",
    "    line_L100_C200_k, line_L100_C200_score = get_line(L, 200, df)\n",
    "    line_L100_C400_k, line_L100_C400_score = get_line(L, 400, df)\n",
    "    \n",
    "    ax1.plot(line_L100_C0_1_k, line_L100_C0_1_score, linestyle=':', label='cov=0.1',color='k')\n",
    "    ax1.plot(line_L100_C1_k, line_L100_C1_score, linestyle='--', label='cov=1',color='k')\n",
    "    ax1.plot(line_L100_C10_k, line_L100_C10_score, linestyle='-', label='cov=10',color='gray')\n",
    "    ax1.plot(line_L100_C100_k, line_L100_C100_score, linestyle='--', label='cov=100',color='gray')\n",
    "    ax1.plot(line_L100_C200_k, line_L100_C200_score, linestyle='-.', label='cov=200',color='k')\n",
    "    ax1.plot(line_L100_C400_k, line_L100_C400_score, linestyle='-', label='cov=400',color='k')\n",
    "\n",
    "    ax1.set_xticks([0, 1, 2,4,6,8,10,12])\n",
    "    ax1.legend()\n",
    "    ax1.set_title('L=100')\n",
    "    ax1.set_ylabel('average species-level recall')\n",
    "    ax1.set_xlabel('k-mer size')\n",
    "    ax1.grid(color='lightgray', linestyle=':', linewidth=1)\n",
    "    \n",
    "    # graph 2\n",
    "    L=200\n",
    "    line_L200_C0_1_k, line_L200_C0_1_score = get_line(L, 0.1, df)\n",
    "    line_L200_C1_k, line_L200_C1_score = get_line(L, 1, df)\n",
    "    line_L200_C10_k, line_L200_C10_score = get_line(L, 10, df)\n",
    "    line_L200_C100_k, line_L200_C100_score = get_line(L, 100, df)\n",
    "    line_L200_C200_k, line_L200_C200_score = get_line(L, 200, df)\n",
    "    line_L200_C400_k, line_L200_C400_score = get_line(L, 400, df)\n",
    "    \n",
    "    \n",
    "    ax2.plot(line_L200_C0_1_k, line_L200_C0_1_score, linestyle=':', label='cov=0.1',color='k')\n",
    "    ax2.plot(line_L200_C1_k, line_L200_C1_score, linestyle='--', label='cov=1',color='k')\n",
    "    ax2.plot(line_L200_C10_k, line_L200_C10_score, linestyle='-', label='cov=10',color='gray')\n",
    "    ax2.plot(line_L200_C100_k, line_L200_C100_score, linestyle='--', label='cov=100',color='gray')\n",
    "    ax2.plot(line_L200_C200_k, line_L200_C200_score, linestyle='-.', label='cov=200',color='k')\n",
    "    ax2.plot(line_L200_C400_k, line_L200_C400_score, linestyle='-', label='cov=400',color='k')\n",
    "\n",
    "    ax2.set_xticks([0, 1, 2,4,6,8,10,12])\n",
    "    ax2.legend()\n",
    "    ax2.set_title('L=200')\n",
    "    ax2.set_ylabel('average species-level recall')\n",
    "    ax2.set_xlabel('k-mer size')\n",
    "    ax2.grid(color='lightgray', linestyle=':', linewidth=1)\n",
    "    \n",
    "    # graph 3\n",
    "    L=400\n",
    "    line_L400_C0_1_k, line_L400_C0_1_score = get_line(L, 0.1, df)\n",
    "    line_L400_C1_k, line_L400_C1_score = get_line(L, 1, df)\n",
    "    line_L400_C10_k, line_L400_C10_score = get_line(L, 10, df)\n",
    "    line_L400_C100_k, line_L400_C100_score = get_line(L, 100, df)\n",
    "    line_L400_C200_k, line_L400_C200_score = get_line(L, 200, df)\n",
    "    line_L400_C400_k, line_L400_C400_score = get_line(L, 400, df)\n",
    "\n",
    "    \n",
    "    ax3.plot(line_L400_C0_1_k, line_L400_C0_1_score, linestyle=':', label='cov=0.1',color='k')\n",
    "    ax3.plot(line_L400_C1_k, line_L400_C1_score, linestyle='--', label='cov=1',color='k')\n",
    "    ax3.plot(line_L400_C10_k, line_L400_C10_score, linestyle='-', label='cov=10',color='gray')\n",
    "    ax3.plot(line_L400_C100_k, line_L400_C100_score, linestyle='--', label='cov=100',color='gray')\n",
    "    ax3.plot(line_L400_C200_k, line_L400_C200_score, linestyle='-.', label='cov=200',color='k')\n",
    "    ax3.plot(line_L400_C400_k, line_L400_C400_score, linestyle='-', label='cov=400',color='k')\n",
    "\n",
    "    ax3.set_xticks([0, 1, 2,4,6,8,10,12])\n",
    "    ax3.legend()\n",
    "    ax3.set_title('L=400')\n",
    "    ax3.set_ylabel('average species-level recall')\n",
    "    ax3.set_xlabel('k-mer size')\n",
    "    ax3.grid(color='lightgray', linestyle=':', linewidth=1)\n",
    "    \n",
    "    plt.suptitle(\"Random Forest Classifier (max_depth=15, n_estimators=50)\", fontsize=16)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-observation",
   "metadata": {},
   "source": [
    "### Search 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "premium-roulette",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1, 1, 2, 10]\n",
    "# list_k = [i for i in range(1,10,2)]\n",
    "# list_max_depth = [i for i in range(2,20,4)]\n",
    "# list_n_estimators = [i for i in range(20,200,40)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparative-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.07.15.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.44909382e-04,  1.67645287e-02, -9.17412031e-03,  1.18482770e-03, -1.44167915e-05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-questionnaire",
   "metadata": {},
   "source": [
    "### Search 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "processed-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# 3h 37min 3s\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200, 400]\n",
    "# list_coverage = [1,10,100,200]\n",
    "# list_k = [i for i in range(1,20,2)]\n",
    "# list_max_depth = [i for i in range(6,20,2)]\n",
    "# list_n_estimators = [i for i in range(50,501,50)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "defined-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.31.24.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.79147764e-04,  8.02651943e-04, -1.15739138e-02,  2.46285125e-03, 4.98644276e-06])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-antarctica",
   "metadata": {},
   "source": [
    "### Search 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "upset-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 2h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.3'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1,1,10,100,200]\n",
    "# list_k = [1,2,4,8]\n",
    "# list_max_depth = [i for i in range(20,51,10)]\n",
    "# list_n_estimators = [50,100,200,400,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-apache",
   "metadata": {},
   "source": [
    "### Search 1.4\n",
    "Stopped short after realizing I didn't need to try as small intervals of max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "realistic-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.4'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [50]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-alloy",
   "metadata": {},
   "source": [
    "### Search 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "genuine-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 5.5 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.5'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6]\n",
    "# list_max_depth = [50, 100, 200]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-orange",
   "metadata": {},
   "source": [
    "### Search 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "short-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #2.25 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.6'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,3]\n",
    "# list_max_depth = [55, 65, 75, 85, 95]\n",
    "# list_n_estimators = [400,500,600,700,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "russian-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi-all.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.95530967e-04,  1.27461350e-03, -1.10693995e-02,  1.45827089e-03, 4.77729253e-05])\n",
    "# # x1000 =\n",
    "# # -0.19  sample length\n",
    "# #  1.27  coverage\n",
    "# # -10    k\n",
    "# #  1.45  max_depth\n",
    "# #  0.047 n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-cambodia",
   "metadata": {},
   "source": [
    "### Search 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "small-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.7'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,501, 50)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-uganda",
   "metadata": {},
   "source": [
    "### Search 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spanish-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 9 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.8'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,101, 10)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-pastor",
   "metadata": {},
   "source": [
    "### Search 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "institutional-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 51 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.9'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(10,101, 2)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-circus",
   "metadata": {},
   "source": [
    "### Search 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "private-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 16 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.10'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(2,103,10)] + [i for i in range(100,1001,100)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-charter",
   "metadata": {},
   "source": [
    "### Search 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "handed-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.11'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(200,501,25)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-retailer",
   "metadata": {},
   "source": [
    "### Search 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eligible-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1h\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.12'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12,14,16,18,20]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-grounds",
   "metadata": {},
   "source": [
    "### Search 1.13\n",
    "Searching over smaller max_depth and n_estimator space due to limitations as we scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "governmental-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 22 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.13'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [i for i in range(5,30,5)]\n",
    "# list_n_estimators = [i for i in range(20,50,5)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-scope",
   "metadata": {},
   "source": [
    "### Search 1.14\n",
    "Switching to species-level recall and rechecking highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "demonstrated-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '1.14'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [400]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-raising",
   "metadata": {},
   "source": [
    "# Grid Search 2 - 10000 size dataset (198 examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-accident",
   "metadata": {},
   "source": [
    "### Search 2.01\n",
    "Determine if method even works with new data.\n",
    "Result: takes a long time to complete even a single run with 500 estimators and 72 max depth. May need to reduce one or both - reconsider how grid search 1 performs with higher coverage but lower estimators / depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "suspected-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 47 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.01'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-mitchell",
   "metadata": {},
   "source": [
    "### Search 2.02\n",
    "10x less estimators. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "professional-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 5 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.02'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-boulder",
   "metadata": {},
   "source": [
    "### Search 2.03\n",
    "10x less depth. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "false-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.03'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [7]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-anthony",
   "metadata": {},
   "source": [
    "### Search 2.04\n",
    "Stopped early because we don't need to try k values higher than 4 because the accuracy is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extensive-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.04'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-milan",
   "metadata": {},
   "source": [
    "### Search 2.05\n",
    "Stopped after first run at 400x sample coverage. Took at least 10 hours for that run alone, maybe longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "powerful-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.05'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200,400]\n",
    "# list_coverage = [10,50,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-barrier",
   "metadata": {},
   "source": [
    "### Search 2.06\n",
    "Testing with small max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "round-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 10 h 15 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.06'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-travel",
   "metadata": {},
   "source": [
    "# Real Runs - 10000 size dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-scanner",
   "metadata": {},
   "source": [
    "### Run 3.01\n",
    "Testing with small max_depth and n_estimators to get initial round of data for diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "biological-kingdom",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 11 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.01'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [0.1,1,10,100,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-capacity",
   "metadata": {},
   "source": [
    "### Run 3.02\n",
    "Run for coverage=200 using previous configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lined-center",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 4h 20 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.02'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "flying-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename='data/runs-10000/rf-multi.combined.csv'\n",
    "# plot_graphs(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-campaign",
   "metadata": {},
   "source": [
    "### Run 3.03\n",
    "Run with higher max_depth to check difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-humor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 400)\n",
      "Percent complete: 0.7936507936507936\n",
      "(1339, 800)\n",
      "Percent complete: 1.5873015873015872\n",
      "(1339, 6244)\n",
      "Percent complete: 2.380952380952381\n",
      "(1339, 16836)\n",
      "Percent complete: 3.1746031746031744\n",
      "(1339, 15665)\n",
      "Percent complete: 3.968253968253968\n",
      "(1339, 13324)\n",
      "Percent complete: 4.761904761904762\n",
      "(1339, 10676)\n",
      "Percent complete: 5.555555555555555\n",
      "(12432, 400)\n",
      "Percent complete: 6.349206349206349\n",
      "(12432, 800)\n",
      "Percent complete: 7.142857142857142\n",
      "(12432, 6400)\n",
      "Percent complete: 7.936507936507936\n",
      "(12432, 56659)\n",
      "Percent complete: 8.73015873015873\n",
      "(12432, 121248)\n",
      "Percent complete: 9.523809523809524\n",
      "(12432, 117510)\n",
      "Percent complete: 10.317460317460316\n",
      "(12432, 95431)\n",
      "Percent complete: 11.11111111111111\n",
      "(123485, 400)\n",
      "Percent complete: 11.904761904761903\n",
      "(123485, 800)\n",
      "Percent complete: 12.698412698412698\n",
      "(123485, 6400)\n",
      "Percent complete: 13.492063492063492\n",
      "(123485, 65534)\n",
      "Percent complete: 14.285714285714285\n",
      "(123485, 509745)\n",
      "Percent complete: 15.079365079365079\n",
      "(123485, 852719)\n",
      "Percent complete: 15.873015873015872\n",
      "(123485, 753836)\n",
      "Percent complete: 16.666666666666664\n",
      "(1233974, 400)\n",
      "Percent complete: 17.46031746031746\n",
      "(1233974, 800)\n",
      "Percent complete: 18.253968253968253\n",
      "(1233974, 6400)\n",
      "Percent complete: 19.047619047619047\n",
      "(1233974, 65536)\n",
      "Percent complete: 19.841269841269842\n",
      "(1233974, 764793)\n",
      "Percent complete: 20.634920634920633\n",
      "(1233974, 3134454)\n",
      "Percent complete: 21.428571428571427\n",
      "(1233974, 3502665)\n",
      "Percent complete: 22.22222222222222\n",
      "(2467948, 400)\n",
      "Percent complete: 23.015873015873016\n",
      "(2467948, 800)\n",
      "Percent complete: 23.809523809523807\n",
      "(2467948, 6400)\n",
      "Percent complete: 24.6031746031746\n",
      "(2467948, 65536)\n",
      "Percent complete: 25.396825396825395\n",
      "(2467948, 775326)\n",
      "Percent complete: 26.190476190476193\n",
      "(2467948, 3710791)\n",
      "Percent complete: 26.984126984126984\n",
      "(2467948, 4395975)\n",
      "Percent complete: 27.77777777777778\n",
      "(4935896, 400)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parameters\n",
    "seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "output_dir = 'data/sampling/sampling-toy-10000'\n",
    "pattern = 'fragments*.npy'\n",
    "seed = None\n",
    "date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "experiment = '3.03'\n",
    "score_type = 'species_recall'\n",
    "\n",
    "# combinations to try\n",
    "list_sample_length = [100,200,400]\n",
    "list_coverage = [0.1,1,10,100,200, 400]\n",
    "list_k = [1,2,4,6,8,10,12]\n",
    "list_max_depth = [30]\n",
    "list_n_estimators = [50]\n",
    "\n",
    "\n",
    "grid_search_multiclass_rf(seq_file, \n",
    "                          taxid_file, \n",
    "                          output_dir, \n",
    "                          pattern, \n",
    "                          list_sample_length, \n",
    "                          list_coverage, \n",
    "                          list_k, \n",
    "                          list_max_depth,\n",
    "                          list_n_estimators, \n",
    "                          seed,\n",
    "                          grid_search_file,\n",
    "                          fields,\n",
    "                          experiment,\n",
    "                          score_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
