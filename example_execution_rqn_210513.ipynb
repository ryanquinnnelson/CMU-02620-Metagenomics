{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italian-receptor",
   "metadata": {},
   "source": [
    "This is an example run of implemented models for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-museum",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "further-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# custom packages\n",
    "from packages.metagenomics import sampling2, encoding2\n",
    "from packages.linear_model.MulticlassLogisticRegression import MulticlassLogisticRegression\n",
    "from packages.generative_model.naive_bayes import run_naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-newport",
   "metadata": {},
   "source": [
    "## Sampling and Encoding the fragment dataset\n",
    "Metagenomics data must be standardized for use in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-croatia",
   "metadata": {},
   "source": [
    "### Fragment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fancy-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building fragments...\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "output_dir = 'data/example/2000-lengths-dataset'\n",
    "seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "sample_length = 200\n",
    "coverage = 400\n",
    "seed = 42\n",
    "\n",
    "# delete output directory if it previously exists\n",
    "try:\n",
    "    shutil.rmtree(output_dir)\n",
    "except FileNotFoundError:\n",
    "    print('Existing directory was not found. Process will generate a directory.')\n",
    "\n",
    "# build fragments\n",
    "print('Building fragments...')\n",
    "sampling2.generate_fragment_data(seq_file, taxid_file, output_dir, sample_length, coverage, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-bridge",
   "metadata": {},
   "source": [
    "### Fragment Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wooden-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded fragments...\n",
      "(16282, 88349)\n",
      "Encoding succeeded.\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "pattern = 'fragments*.npy'\n",
    "k = 6\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# encode data and labels\n",
    "fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "X_enc, y = encoding2.encode_fragment_dataset(fragments, k)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "print('Encoded fragments...')\n",
    "print(X_enc.shape)\n",
    "\n",
    "# perform check so that randomly split training and test sets both contain all classes in the data\n",
    "n_classes = len(np.unique(y_enc))\n",
    "n_classes_train = 0\n",
    "n_classes_test = 0\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "count = 0\n",
    "while n_classes_train < n_classes or n_classes_test < n_classes:\n",
    "    if n_classes_train != 0:\n",
    "        print('Encoding failed')\n",
    "\n",
    "    # split data into test and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.33, random_state=seed)\n",
    "    n_classes_train = len(np.unique(y_train))\n",
    "    n_classes_test = len(np.unique(y_test))\n",
    "    count += 1\n",
    "\n",
    "    if count > 1000:\n",
    "        # there must be an issue and we are stuck in an infinite loop\n",
    "        msg = 'Not possible for both training and test sets to contain all classes.'\n",
    "        msg2 = ' (n_classes, training set length, test set length):'\n",
    "        raise ValueError(msg + msg2 + str(n_classes), len(y_train), len(y_test))\n",
    "\n",
    "print('Encoding succeeded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-professional",
   "metadata": {},
   "source": [
    "## Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unusual-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "0.8987718645329363\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "eta = 0.1\n",
    "epsilon = 0.01\n",
    "penalty = None\n",
    "l2_lambda = 0\n",
    "max_iter = 200\n",
    "\n",
    "\n",
    "# train model\n",
    "mlr = MulticlassLogisticRegression(eta=eta,\n",
    "                                   epsilon=epsilon,\n",
    "                                   penalty=penalty,\n",
    "                                   l2_lambda=l2_lambda,\n",
    "                                   max_iter=max_iter,\n",
    "                                   verbose=True)\n",
    "mlr.fit(X_train, y_train)\n",
    "y_pred = mlr.predict(X_test)\n",
    "score = recall_score(y_test, y_pred, average='weighted')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-trader",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grave-disease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8639746929661333\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "score = run_naive_bayes(X_train, X_test, y_train, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
