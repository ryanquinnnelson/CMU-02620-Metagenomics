{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "similar-slovenia",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import csv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from packages.metagenomics import sampling2, encoding2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from packages.LogisticRegression.MulticlassLogisticRegression import MulticlassLogisticRegression,MulticlassLogisticRegression2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "linear-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results_to_file(filename, fields=None, rows=None):\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        if fields:\n",
    "            write.writerow(fields)\n",
    "\n",
    "        if rows:\n",
    "            write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contained-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlr_classification_recall(X_train, X_test, y_train, y_test, eta, epsilon):\n",
    "    \"\"\"\n",
    "    Score is species level recall.\n",
    "    \"\"\"\n",
    "    mlr = MulticlassLogisticRegression2(eta=eta, epsilon=epsilon)\n",
    "    mlr.fit(X_train,y_train)\n",
    "    y_pred = mlr.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sonic-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed):\n",
    "    # delete output directory if it previously exists\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except FileNotFoundError:\n",
    "        print('Existing directory was not found. Process will generate a directory.')\n",
    "\n",
    "    # build fragments\n",
    "    sampling2.generate_fragment_data(seq_file, taxid_file, output_dir, sample_length, coverage, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moved-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fragments(output_dir, pattern, k, seed):\n",
    "    \n",
    "    # encode data\n",
    "    fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "    X_enc, y = encoding2.encode_fragment_dataset(fragments,k)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    \n",
    "    # calculate number of classes\n",
    "    n_classes = len(np.unique(y_enc))\n",
    "#     print('n_classes:',n_classes)\n",
    "    n_classes_train = 0\n",
    "    n_classes_test = 0\n",
    "    while n_classes_train < n_classes or n_classes_test < n_classes:\n",
    "\n",
    "        # split data into test and training\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_enc.toarray(), y_enc, test_size=0.33)\n",
    "        n_classes_train = len(np.unique(y_train))\n",
    "        n_classes_test = len(np.unique(y_test))\n",
    "#         print('train:',len(y_train))\n",
    "#         print('test:', len(y_test))\n",
    "    \n",
    "    print(X_enc.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fatal-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations(list_sample_length,list_coverage,list_k,list_eta,list_epsilon):\n",
    "    n = len(list_sample_length) * len(list_coverage) * len(list_k) *len(list_eta) * len(list_epsilon)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dramatic-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_mlr(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_eta,\n",
    "                              list_epsilon, \n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                              score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations(list_sample_length,list_coverage,list_k,list_eta,list_epsilon)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    for sample_length in list_sample_length:\n",
    "        for coverage in list_coverage:\n",
    "            \n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            for k in list_k:\n",
    "                \n",
    "                # kmer combination\n",
    "                X_train, X_test, y_train, y_test = encode_fragments(output_dir, pattern,k,seed)\n",
    "                for eta in list_eta:\n",
    "                    for epsilon in list_epsilon:\n",
    "                        \n",
    "                        # random forest combination\n",
    "                        score = run_mlr_classification_recall(X_train, X_test, y_train,y_test, eta, epsilon)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # output results to file\n",
    "                        row = [experiment, 'multiclass', 'Logistic Regression', X_train.shape, sample_length, coverage, k, eta, epsilon, score, score_type]\n",
    "                        append_results_to_file(grid_search_file, row)\n",
    "                        \n",
    "                print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-italic",
   "metadata": {},
   "source": [
    "# Run Set 1 - MLR Toy\n",
    "2000 lengths dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-terrorist",
   "metadata": {},
   "source": [
    "### Run 4.01\n",
    "Stopped early due to runs taking a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "organizational-station",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'score','score type']\n",
    "# experiment = '4.01'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [0.2,1,10]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_eta = [0.01]\n",
    "# list_epsilon = [0.01]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_mlr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_eta,\n",
    "#                           list_epsilon,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-somalia",
   "metadata": {},
   "source": [
    "### Run 4.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-halifax",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'score','score type']\n",
    "# experiment = '4.02'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200]\n",
    "# list_coverage = [0.3,1,10]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_eta = [0.01]\n",
    "# list_epsilon = [0.01]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_mlr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_eta,\n",
    "#                           list_epsilon,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-cincinnati",
   "metadata": {},
   "source": [
    "### Run 4.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conscious-spending",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'score','score type']\n",
    "# experiment = '4.03'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [400]\n",
    "# list_coverage = [0.5,1,10]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_eta = [0.01]\n",
    "# list_epsilon = [0.01]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_mlr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_eta,\n",
    "#                           list_epsilon,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-vietnam",
   "metadata": {},
   "source": [
    "### Run 4.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "august-michael",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'score','score type']\n",
    "# experiment = '4.04'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [100]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_eta = [0.01]\n",
    "# list_epsilon = [0.01]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_mlr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_eta,\n",
    "#                           list_epsilon,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-signal",
   "metadata": {},
   "source": [
    "### Run 4.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hawaiian-adventure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "##40 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'score','score type']\n",
    "# experiment = '4.05'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_eta = [0.01]\n",
    "# list_epsilon = [0.01]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_mlr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_eta,\n",
    "#                           list_epsilon,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-hello",
   "metadata": {},
   "source": [
    "# Run Set 2 - sklearn with l1 penalty\n",
    "Compare with sklearn implementation of MLR to see if performance is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "catholic-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations2(list_sample_length,list_coverage,list_k,list_multiclass,list_classweight):\n",
    "    n = len(list_sample_length) * len(list_coverage) * len(list_k) *len(list_multiclass) * len(list_classweight)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sharing-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fragments2(output_dir, pattern, k, seed):\n",
    "    \"\"\"\n",
    "    Does not convert sparse matrix to numpy matrix first.\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode data\n",
    "    fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "    X_enc, y = encoding2.encode_fragment_dataset(fragments,k)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    \n",
    "    # calculate number of classes\n",
    "    n_classes = len(np.unique(y_enc))\n",
    "#     print('n_classes:',n_classes)\n",
    "    n_classes_train = 0\n",
    "    n_classes_test = 0\n",
    "    while n_classes_train < n_classes or n_classes_test < n_classes:\n",
    "\n",
    "        # split data into test and training\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.33)\n",
    "        n_classes_train = len(np.unique(y_train))\n",
    "        n_classes_test = len(np.unique(y_test))\n",
    "#         print('train:',y_train)\n",
    "#         print('test:', y_test)\n",
    "    \n",
    "    print(X_enc.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "outdoor-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lr_classification_recall(X_train, X_test, y_train, y_test, multiclass, classweight, seed ):\n",
    "    \"\"\"\n",
    "    Score is species level recall. Uses sklearn version of logistic regression.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(random_state=seed, multi_class=multiclass, class_weight=classweight )\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flying-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_lr(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_multiclass,\n",
    "                              list_classweight,\n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                              score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations2(list_sample_length,list_coverage,list_k, list_multiclass, list_classweight)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    for sample_length in list_sample_length:\n",
    "        for coverage in list_coverage:\n",
    "            \n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            for k in list_k:\n",
    "                \n",
    "                # kmer combination\n",
    "                X_train, X_test, y_train, y_test = encode_fragments2(output_dir, pattern,k,seed)\n",
    "                \n",
    "                \n",
    "                for multiclass in list_multiclass:\n",
    "                    for classweight in list_classweight:\n",
    "\n",
    "                        \n",
    "                        # random forest combination\n",
    "                        score = run_lr_classification_recall(X_train, X_test, y_train, y_test, multiclass, classweight, seed)\n",
    "                        count += 1\n",
    "\n",
    "                        # output results to file\n",
    "                        row = [experiment, 'multiclass', 'Logistic Regression (sklearn)', X_train.shape, sample_length, coverage, k, multiclass, classweight, score, score_type]\n",
    "                        append_results_to_file(grid_search_file, row)\n",
    "\n",
    "                print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-extraction",
   "metadata": {},
   "source": [
    "### Run 5.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stupid-islam",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/lr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','multiclass', 'class_weight', 'score','score type']\n",
    "# experiment = '5.01'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [1,10,100,200,400]\n",
    "# list_k = [1]\n",
    "# list_multiclass = ['auto']\n",
    "# list_classweight= [None]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_lr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_multiclass,\n",
    "#                           list_classweight,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-newton",
   "metadata": {},
   "source": [
    "### Run 5.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "romantic-roots",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-2000/lr-multi.{}.csv'.format(date_time)\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','multiclass', 'class_weight', 'score','score type']\n",
    "# experiment = '5.02'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [1,10,100,200,400]\n",
    "# list_k = [2,4,6,8,10,12]\n",
    "# list_multiclass = ['auto']\n",
    "# list_classweight= [None]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_lr(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_multiclass,\n",
    "#                           list_classweight,\n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-disorder",
   "metadata": {},
   "source": [
    "# Run Set 3 - MLR with L2 penalty\n",
    "- toy-2000 dataset\n",
    "- See how performance changes with L2 penalty\n",
    "- Changed breakpoint in gradient descent to 100 from 100,000 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations(*args):\n",
    "    total = 1\n",
    "    for each in args:\n",
    "        total *= len(each)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_generator(list_sample_length,list_coverage,list_k):\n",
    "    \n",
    "    for L in list_sample_length:\n",
    "        for c in list_coverage:\n",
    "            for k in list_k:\n",
    "                yield L, c, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_generator(list_eta,list_epsilon, list_penalty, list_l2_lambda,list_max_iter):\n",
    "    \n",
    "    for eta in list_eta:\n",
    "        for e in list_epsilon:\n",
    "            for penalty in list_penalty:\n",
    "                for l2 in list_l2_lambda:\n",
    "                    for m in list_max_iter:\n",
    "                        yield eta,e,penalty,l2,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "natural-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlr_classification_recall(X_train, X_test, y_train, y_test, eta, epsilon, penalty, l2_lambda, max_iter):\n",
    "    \"\"\"\n",
    "    Score is species level recall.\n",
    "    \"\"\"\n",
    "    mlr = MulticlassLogisticRegression2(eta=eta, \n",
    "                                        epsilon=epsilon, \n",
    "                                        penalty=penalty, \n",
    "                                        l2_lambda=l2_lambda, \n",
    "                                        max_iter=max_iter)\n",
    "    mlr.fit(X_train,y_train)\n",
    "    y_pred = mlr.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "supreme-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_mlr(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_eta,\n",
    "                              list_epsilon, \n",
    "                              list_l2_lambda,\n",
    "                              list_max_iter,\n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                              score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations(list_sample_length,\n",
    "                                              list_coverage,\n",
    "                                              list_k,\n",
    "                                              list_eta,\n",
    "                                              list_epsilon, \n",
    "                                              list_l2_lambda, \n",
    "                                              list_max_iter)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    sample_length_prev = -1\n",
    "    coverage_prev = -1\n",
    "    \n",
    "    # parameter combinations\n",
    "    for sample_length, coverage,k in parameter_generator(list_sample_length,list_coverage,list_k):\n",
    "        print(sample_length, coverage,k)\n",
    "        \n",
    "        if sample_length != sample_length_prev or coverage != coverage_prev:\n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            print('building new fragments set')\n",
    "                \n",
    "        # kmer combination\n",
    "        X_train, X_test, y_train, y_test = encode_fragments(output_dir, pattern,k,seed)\n",
    "        \n",
    "        \n",
    "        # hyperparameter combinations\n",
    "        for eta, epsilon, penalty, l2_lambda, max_iter in hyperparameter_generator(list_eta,list_epsilon, list_penalty, list_l2_lambda,list_max_iter):\n",
    "            print(eta, epsilon, penalty, l2_lambda, max_iter)\n",
    "            # train and score model\n",
    "            score = run_mlr_classification_recall(X_train, X_test, y_train, y_test, eta, epsilon, penalty, l2_lambda, max_iter)\n",
    "            count += 1\n",
    "\n",
    "            # output results to file\n",
    "            row = [experiment, 'multiclass', 'Logistic Regression', X_train.shape, sample_length, coverage, k, eta, epsilon, penalty, l2_lambda, max_iter, score, score_type]\n",
    "            append_results_to_file(grid_search_file, row)\n",
    "                        \n",
    "        print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-partnership",
   "metadata": {},
   "source": [
    "### Run 6.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-incident",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "Percent complete: 3.8095238095238098\n",
      "(85, 793)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "Percent complete: 7.6190476190476195\n",
      "(85, 1722)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 11.428571428571429\n",
      "(85, 1338)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 15.238095238095239\n",
      "(85, 1018)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 19.047619047619047\n",
      "(85, 850)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 22.857142857142858\n",
      "(85, 680)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 26.666666666666668\n",
      "(816, 400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 30.476190476190478\n",
      "(816, 800)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 34.285714285714285\n",
      "(816, 5689)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 38.095238095238095\n",
      "(816, 10714)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 41.904761904761905\n",
      "(816, 9328)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 45.714285714285715\n",
      "(816, 7885)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 49.523809523809526\n",
      "(816, 6317)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 53.333333333333336\n",
      "(8141, 400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/MulticlassLogisticRegression.py:176: RuntimeWarning: invalid value encountered in true_divide\n",
      "  standardized = y_pred_proba / sums\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 57.14285714285714\n",
      "(8141, 800)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 60.952380952380956\n",
      "(8141, 6399)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 64.76190476190476\n",
      "(8141, 38665)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 68.57142857142857\n",
      "(8141, 53604)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 72.38095238095238\n",
      "(8141, 48759)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 76.19047619047619\n",
      "(8141, 39395)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 80.0\n",
      "(16282, 400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "training classifier 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 83.80952380952381\n",
      "(16282, 800)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 87.61904761904762\n",
      "(16282, 6400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 91.42857142857143\n",
      "(16282, 43939)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 95.23809523809523\n",
      "(16282, 68550)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 99.04761904761905\n",
      "(16282, 63875)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 102.85714285714285\n",
      "(16282, 51766)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 106.66666666666667\n",
      "(32564, 400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:289: RuntimeWarning: overflow encountered in exp\n",
      "  inner = ones + np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:241: RuntimeWarning: overflow encountered in exp\n",
      "  top = np.exp(Xw)\n",
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/gradient_descent.py:244: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return top / bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier 3\n",
      "training classifier 4\n",
      "Percent complete: 110.47619047619048\n",
      "(32564, 800)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/GitHub/C-A-L-C-I-F-E-R/CMU-02620-Metagenomics/packages/LogisticRegression/MulticlassLogisticRegression.py:176: RuntimeWarning: invalid value encountered in true_divide\n",
      "  standardized = y_pred_proba / sums\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 114.28571428571428\n",
      "(32564, 6400)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 118.0952380952381\n",
      "(32564, 46691)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "training classifier 1\n",
      "training classifier 2\n",
      "training classifier 3\n",
      "training classifier 4\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 2\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 3\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 4\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "Percent complete: 121.90476190476191\n",
      "(32564, 77733)\n",
      "n_classifiers 5\n",
      "training classifier 0\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "training classifier 1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parameters\n",
    "seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "output_dir = 'data/sampling/sampling-toy-2000-mlr'\n",
    "pattern = 'fragments*.npy'\n",
    "seed = 42\n",
    "date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "grid_search_file  = 'data/runs-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','eta', 'epsilon', 'l2_penalty','score','score type']\n",
    "experiment = '6.01'\n",
    "score_type = 'species_recall'\n",
    "\n",
    "# combinations to try\n",
    "list_sample_length = [100]\n",
    "list_coverage = [1,10,100,200,400]\n",
    "list_k = [1,2,4,6,8,10,12]\n",
    "list_eta = [0.01]\n",
    "list_epsilon = [0.01]\n",
    "list_l2_penalty = [1,10,100, 0.1]\n",
    "\n",
    "\n",
    "grid_search_multiclass_mlr_l2(seq_file, \n",
    "                          taxid_file, \n",
    "                          output_dir, \n",
    "                          pattern, \n",
    "                          list_sample_length, \n",
    "                          list_coverage, \n",
    "                          list_k, \n",
    "                          list_eta,\n",
    "                          list_epsilon,\n",
    "                          list_l2_penalty,\n",
    "                          seed,\n",
    "                          grid_search_file,\n",
    "                          fields,\n",
    "                          experiment,\n",
    "                          score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-insider",
   "metadata": {},
   "source": [
    "### Run 6.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# parameters\n",
    "seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "output_dir = 'data/sampling/sampling-toy-2000'\n",
    "pattern = 'fragments*.npy'\n",
    "seed = 42\n",
    "date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "grid_search_file  = 'data/gridsearch-2000/mlr-multi.{}.csv'.format(date_time)\n",
    "fields = ['experiment',\n",
    "          'category',\n",
    "          'classifier',\n",
    "          'training shape',\n",
    "          'sample_length',\n",
    "          'coverage',\n",
    "          'k',\n",
    "          'eta', \n",
    "          'epsilon', \n",
    "          'penalty',\n",
    "          'l2_lambda',\n",
    "          'max_iter',\n",
    "          'score',\n",
    "          'score type']\n",
    "\n",
    "experiment = '6.02'\n",
    "score_type = 'species_recall'\n",
    "\n",
    "# combinations to try\n",
    "list_sample_length = [100,200]\n",
    "list_coverage = [1]\n",
    "list_k = [1,2]\n",
    "list_eta = [0.01]\n",
    "list_epsilon = [0.01]\n",
    "list_penalty = ['l2']\n",
    "list_l2_lambda = [0.1,1]\n",
    "list_max_iter = [100]\n",
    "\n",
    "\n",
    "grid_search_multiclass_mlr_l2(seq_file, \n",
    "                                taxid_file, \n",
    "                                output_dir, \n",
    "                                pattern, \n",
    "                                list_sample_length, \n",
    "                                list_coverage, \n",
    "                                list_k, \n",
    "                                list_eta,\n",
    "                                list_epsilon,\n",
    "                                list_penalty,\n",
    "                                list_l2_lambda,\n",
    "                                list_max_iter,\n",
    "                                seed,\n",
    "                                grid_search_file,\n",
    "                                fields,\n",
    "                                experiment,\n",
    "                                score_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
