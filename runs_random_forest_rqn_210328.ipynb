{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "processed-waterproof",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oriented-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.metagenomics import sampling2, encoding2\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import csv\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-warehouse",
   "metadata": {},
   "source": [
    "### Ideas for model improvement\n",
    "- try one-vs-all approach\n",
    "- encode sequences differently depending on whether sequence is a promoter or gene vs CpG islands, etc. Search for known motifs rather than blind k-mer groups.\n",
    "- use PCA/feature selection to reduce dimensions before learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-paintball",
   "metadata": {},
   "source": [
    "# Recall score\n",
    "From the paper: \"Performance is measured in terms of species-level recall. We first compute the prediction recall within each species, i.e. the proportion of fragments originating from this species that are correctly classified and consider the average recall observed across species.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 1, 1]\n",
    "\n",
    "recall_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-portal",
   "metadata": {},
   "source": [
    "# Grid Search 1 - 2000 size dataset (4 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-native",
   "metadata": {},
   "source": [
    "### Run Grid Search 1 - Multiclass Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verified-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results_to_file(filename, fields=None, rows=None):\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        if fields:\n",
    "            write.writerow(fields)\n",
    "\n",
    "        if rows:\n",
    "            write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "differential-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is subset accuracy.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    score = rf.score(X_test, y_test)\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virtual-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed):\n",
    "    \"\"\"\n",
    "    Score is species level recall.\n",
    "    \"\"\"\n",
    "   \n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, random_state=seed)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    score = recall_score(y_test, y_pred, average='weighted')\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broadband-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed):\n",
    "    # delete output directory if it previously exists\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except FileNotFoundError:\n",
    "        print('Existing directory was not found. Process will generate a directory.')\n",
    "\n",
    "    # build fragments\n",
    "    sampling2.generate_fragment_data(seq_file, taxid_file, output_dir, sample_length, coverage, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tribal-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fragments(output_dir, pattern, k, seed):\n",
    "    \n",
    "    # encode data\n",
    "    fragments = sampling2.read_fragments(output_dir, pattern)\n",
    "    X_enc, y = encoding2.encode_fragment_dataset(fragments,k)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    # split data into test and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.33, random_state=seed)\n",
    "    \n",
    "    print(X_enc.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seven-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators):\n",
    "    n = len(list_sample_length) * len(list_coverage) * len(list_k) *len(list_max_depth) * len(list_n_estimators)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mediterranean-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_multiclass_rf(seq_file, \n",
    "                              taxid_file, \n",
    "                              output_dir, \n",
    "                              pattern, \n",
    "                              list_sample_length, \n",
    "                              list_coverage, \n",
    "                              list_k,\n",
    "                              list_max_depth, \n",
    "                              list_n_estimators, \n",
    "                              seed,\n",
    "                              grid_search_file,\n",
    "                              fields,\n",
    "                              experiment,\n",
    "                             score_type):\n",
    "    \n",
    "    # set up grid search results file\n",
    "    append_results_to_file(grid_search_file, fields)\n",
    "    \n",
    "    # calculate number of combinations\n",
    "    n_combinations = calc_number_combinations(list_sample_length,list_coverage,list_k,list_max_depth,list_n_estimators)\n",
    "    \n",
    "    # process combinations\n",
    "    count = 0\n",
    "    for sample_length in list_sample_length:\n",
    "        for coverage in list_coverage:\n",
    "            \n",
    "            # fragment combination\n",
    "            build_fragments(seq_file, taxid_file, output_dir, sample_length, coverage, seed)\n",
    "            for k in list_k:\n",
    "                \n",
    "                # kmer combination\n",
    "                X_train, X_test, y_train, y_test = encode_fragments(output_dir, pattern,k,seed)\n",
    "                for max_depth in list_max_depth:\n",
    "                    for n_estimators in list_n_estimators:\n",
    "                        \n",
    "                        # random forest combination\n",
    "                        score = run_rf_classification_recall(X_train, X_test, y_train,y_test, max_depth, n_estimators, seed)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # output results to file\n",
    "                        row = [experiment, 'multiclass', 'Random Forest', X_train.shape, sample_length, coverage, k, max_depth, n_estimators, score, score_type]\n",
    "                        append_results_to_file(grid_search_file, row)\n",
    "                        \n",
    "                print('Percent complete: {}'.format(count / n_combinations * 100)) # display progress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "broke-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hyperparameter_relationship(filename):\n",
    "    \"\"\"\n",
    "    Runs logistic regression over hyperparameters to find the regression coefficients.\n",
    "    This should give some indicator of how hyperparameters are affecting the score.\n",
    "    \"\"\"\n",
    "    # read in grid search results\n",
    "    df = pd.read_csv(filename)\n",
    "    X = df.drop(['experiment','score', 'category','classifier'],axis=1)\n",
    "    y = df['score']\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,y)\n",
    "    return lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-tractor",
   "metadata": {},
   "source": [
    "### Search 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rolled-tower",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1, 1, 2, 10]\n",
    "# list_k = [i for i in range(1,10,2)]\n",
    "# list_max_depth = [i for i in range(2,20,4)]\n",
    "# list_n_estimators = [i for i in range(20,200,40)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "satellite-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.07.15.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.44909382e-04,  1.67645287e-02, -9.17412031e-03,  1.18482770e-03, -1.44167915e-05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-sierra",
   "metadata": {},
   "source": [
    "### Search 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latter-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# 3h 37min 3s\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# fields = ['category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200, 400]\n",
    "# list_coverage = [1,10,100,200]\n",
    "# list_k = [i for i in range(1,20,2)]\n",
    "# list_max_depth = [i for i in range(6,20,2)]\n",
    "# list_n_estimators = [i for i in range(50,501,50)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fatal-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi.2021.03.28.18.31.24.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.79147764e-04,  8.02651943e-04, -1.15739138e-02,  2.46285125e-03, 4.98644276e-06])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-fairy",
   "metadata": {},
   "source": [
    "### Search 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "desirable-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 2h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.3'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [0.1,1,10,100,200]\n",
    "# list_k = [1,2,4,8]\n",
    "# list_max_depth = [i for i in range(20,51,10)]\n",
    "# list_n_estimators = [50,100,200,400,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-capacity",
   "metadata": {},
   "source": [
    "### Search 1.4\n",
    "Stopped short after realizing I didn't need to try as small intervals of max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contemporary-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.4'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [50]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-printer",
   "metadata": {},
   "source": [
    "### Search 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sustained-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "## 5.5 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.5'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100, 200, 400]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6]\n",
    "# list_max_depth = [50, 100, 200]\n",
    "# list_n_estimators = [50,100,200,400,800,1600,3200]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-progress",
   "metadata": {},
   "source": [
    "### Search 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bottom-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #2.25 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.6'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,3]\n",
    "# list_max_depth = [55, 65, 75, 85, 95]\n",
    "# list_n_estimators = [400,500,600,700,800]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interesting-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch_file = 'data/gridsearch/rf-multi-all.csv'\n",
    "# calc_hyperparameter_relationship(gridsearch_file)\n",
    "# # array([-1.95530967e-04,  1.27461350e-03, -1.10693995e-02,  1.45827089e-03, 4.77729253e-05])\n",
    "# # x1000 =\n",
    "# # -0.19  sample length\n",
    "# #  1.27  coverage\n",
    "# # -10    k\n",
    "# #  1.45  max_depth\n",
    "# #  0.047 n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-cutting",
   "metadata": {},
   "source": [
    "### Search 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loving-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.7'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,501, 50)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-topic",
   "metadata": {},
   "source": [
    "### Search 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enhanced-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 9 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.8'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(1,101, 10)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-experience",
   "metadata": {},
   "source": [
    "### Search 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "measured-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 51 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.9'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [i for i in range(10,101, 2)]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-ribbon",
   "metadata": {},
   "source": [
    "### Search 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incorporate-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 16 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.10'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(2,103,10)] + [i for i in range(100,1001,100)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-kansas",
   "metadata": {},
   "source": [
    "### Search 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "massive-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 12 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.11'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [200]\n",
    "# list_k = [2]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [i for i in range(200,501,25)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-bible",
   "metadata": {},
   "source": [
    "### Search 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "activated-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1h\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.12'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12,14,16,18,20]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-diameter",
   "metadata": {},
   "source": [
    "### Search 1.13\n",
    "Searching over smaller max_depth and n_estimator space due to limitations as we scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "corrected-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 22 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '1.13'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [i for i in range(5,30,5)]\n",
    "# list_n_estimators = [i for i in range(20,50,5)]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-speaking",
   "metadata": {},
   "source": [
    "### Search 1.14\n",
    "Switching to species-level recall and rechecking highest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "german-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# grid_search_file  = 'data/gridsearch-2000/rf-multi.{}.csv'.format(datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '1.14'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [400]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-asthma",
   "metadata": {},
   "source": [
    "### Search 1.15\n",
    "Run with default parameters on 2000 size set to plot for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "encouraging-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #40 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-2000/results-2000-rf-default.csv'\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score', 'score type']\n",
    "# experiment = '1.15'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [0.1,1,10,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [None]\n",
    "# list_n_estimators = [100]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-framing",
   "metadata": {},
   "source": [
    "### Search 1.16\n",
    "Run with best parameters on 2000 size set to plot for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "green-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 5 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-2000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-2000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-2000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-2000/results-2000-rf-best.csv'\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score', 'score type']\n",
    "# experiment = '1.16'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [0.1,1,10,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-structure",
   "metadata": {},
   "source": [
    "# Grid Search 2 - 10000 size dataset (198 examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-wedding",
   "metadata": {},
   "source": [
    "### Search 2.01\n",
    "Determine if method even works with new data.\n",
    "Result: takes a long time to complete even a single run with 500 estimators and 72 max depth. May need to reduce one or both - reconsider how grid search 1 performs with higher coverage but lower estimators / depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "secure-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 47 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.01'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-bumper",
   "metadata": {},
   "source": [
    "### Search 2.02\n",
    "10x less estimators. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fitted-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 5 min\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.02'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-concern",
   "metadata": {},
   "source": [
    "### Search 2.03\n",
    "10x less depth. Checking to see how time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "welsh-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.03'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [10]\n",
    "# list_k = [1]\n",
    "# list_max_depth = [7]\n",
    "# list_n_estimators = [500]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-attention",
   "metadata": {},
   "source": [
    "### Search 2.04\n",
    "Stopped early because we don't need to try k values higher than 4 because the accuracy is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "figured-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.04'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [20,50,100,200,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-dryer",
   "metadata": {},
   "source": [
    "### Search 2.05\n",
    "Stopped after first run at 400x sample coverage. Took at least 10 hours for that run alone, maybe longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "latter-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.05'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [200,400]\n",
    "# list_coverage = [10,50,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [72]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-silicon",
   "metadata": {},
   "source": [
    "### Search 2.06\n",
    "Testing with small max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "coupled-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 10 h 15 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score']\n",
    "# experiment = '2.06'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [10,100,200,400]\n",
    "# list_k = [1,2,4]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [45]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-culture",
   "metadata": {},
   "source": [
    "# Real Runs - 10000 size dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-diabetes",
   "metadata": {},
   "source": [
    "### Run 3.01\n",
    "Testing with small max_depth and n_estimators to get initial round of data for diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "attended-walnut",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 11 h\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.01'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [0.1,1,10,100,400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-geometry",
   "metadata": {},
   "source": [
    "### Run 3.02\n",
    "Run for coverage=200 using previous configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "neural-emphasis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 4h 20 min\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.02'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [200]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [15]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-radio",
   "metadata": {},
   "source": [
    "### Run 3.03\n",
    "Run with higher max_depth to check difference.\n",
    "\n",
    "```\n",
    "list_sample_length = [100,200,400]\n",
    "list_coverage = [0.1,1,10,100,200, 400]\n",
    "list_k = [1,2,4,6,8,10,12]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "stable-thomas",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.03'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [0.1,1,10,100,200]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [30]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-ireland",
   "metadata": {},
   "source": [
    "### Run 3.04\n",
    "Run with higher max_depth to check difference. Continues runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "victorian-feeling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-10000.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-10000.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-10000'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = None\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/runs-10000/rf-multi.{}.csv'.format(date_time )\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score','score type']\n",
    "# experiment = '3.04'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100]\n",
    "# list_coverage = [400]\n",
    "# list_k = [1,2,4,6,8,10,12]\n",
    "# list_max_depth = [30]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-morris",
   "metadata": {},
   "source": [
    "# Real Runs - 7400 size dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-argentina",
   "metadata": {},
   "source": [
    "### Run 8.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "running-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # parameters\n",
    "# seq_file = 'data/train_small-db_toy-7400.fasta'\n",
    "# taxid_file = 'data/train_small-db_toy-7400.taxid'\n",
    "# output_dir = 'data/sampling/sampling-toy-7400'\n",
    "# pattern = 'fragments*.npy'\n",
    "# seed = 42\n",
    "# date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "# grid_search_file  = 'data/gridsearch-7400/results-7400-rf-default.csv'\n",
    "# fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score', 'score type']\n",
    "# experiment = '8.01'\n",
    "# score_type = 'species_recall'\n",
    "\n",
    "# # combinations to try\n",
    "# list_sample_length = [100,200,400]\n",
    "# list_coverage = [0.1,1,10,100,200,400]\n",
    "# list_k = [1,2,4,6] # ,8,10,12\n",
    "# list_max_depth = [30]\n",
    "# list_n_estimators = [50]\n",
    "\n",
    "\n",
    "# grid_search_multiclass_rf(seq_file, \n",
    "#                           taxid_file, \n",
    "#                           output_dir, \n",
    "#                           pattern, \n",
    "#                           list_sample_length, \n",
    "#                           list_coverage, \n",
    "#                           list_k, \n",
    "#                           list_max_depth,\n",
    "#                           list_n_estimators, \n",
    "#                           seed,\n",
    "#                           grid_search_file,\n",
    "#                           fields,\n",
    "#                           experiment,\n",
    "#                           score_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-washer",
   "metadata": {},
   "source": [
    "# Real Runs - 5000 size dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-treasure",
   "metadata": {},
   "source": [
    "### Run 11.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247, 400)\n",
      "Percent complete: 1.3888888888888888\n",
      "(247, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/opt/anaconda3/envs/cmu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ryanqnelson/opt/anaconda3/envs/cmu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent complete: 2.7777777777777777\n",
      "(247, 3649)\n",
      "Percent complete: 4.166666666666666\n",
      "(247, 3582)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanqnelson/opt/anaconda3/envs/cmu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ryanqnelson/opt/anaconda3/envs/cmu/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent complete: 5.555555555555555\n",
      "(2128, 400)\n",
      "Percent complete: 6.944444444444445\n",
      "(2128, 800)\n",
      "Percent complete: 8.333333333333332\n",
      "(2128, 6376)\n",
      "Percent complete: 9.722222222222223\n",
      "(2128, 23815)\n",
      "Percent complete: 11.11111111111111\n",
      "(21000, 400)\n",
      "Percent complete: 12.5\n",
      "(21000, 800)\n",
      "Percent complete: 13.88888888888889\n",
      "(21000, 6400)\n",
      "Percent complete: 15.277777777777779\n",
      "(21000, 62800)\n",
      "Percent complete: 16.666666666666664\n",
      "(209680, 400)\n",
      "Percent complete: 18.055555555555554\n",
      "(209680, 800)\n",
      "Percent complete: 19.444444444444446\n",
      "(209680, 6400)\n",
      "Percent complete: 20.833333333333336\n",
      "(209680, 65533)\n",
      "Percent complete: 22.22222222222222\n",
      "(419360, 400)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parameters\n",
    "seq_file = 'data/train_small-db_toy-5000.fasta'\n",
    "taxid_file = 'data/train_small-db_toy-5000.taxid'\n",
    "output_dir = 'data/sampling/sampling-toy-5000'\n",
    "pattern = 'fragments*.npy'\n",
    "seed = 42\n",
    "date_time = datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S')\n",
    "grid_search_file  = 'data/gridsearch-5000/results-5000-rf-default.csv'\n",
    "fields = ['experiment','category','classifier','training shape','sample_length','coverage','k','max_depth', 'n_estimators', 'score', 'score type']\n",
    "experiment = '11.01'\n",
    "score_type = 'species_recall'\n",
    "\n",
    "# combinations to try\n",
    "list_sample_length = [100,200,400]\n",
    "list_coverage = [0.1,1,10,100,200,400]\n",
    "list_k = [1,2,4,6] #,8,10,12\n",
    "list_max_depth = [30]\n",
    "list_n_estimators = [50]\n",
    "\n",
    "\n",
    "grid_search_multiclass_rf(seq_file, \n",
    "                          taxid_file, \n",
    "                          output_dir, \n",
    "                          pattern, \n",
    "                          list_sample_length, \n",
    "                          list_coverage, \n",
    "                          list_k, \n",
    "                          list_max_depth,\n",
    "                          list_n_estimators, \n",
    "                          seed,\n",
    "                          grid_search_file,\n",
    "                          fields,\n",
    "                          experiment,\n",
    "                          score_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
